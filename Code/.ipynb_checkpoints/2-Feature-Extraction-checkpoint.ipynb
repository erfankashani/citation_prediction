{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- Create the Tables for Authors, Venues\n",
    "- Perform Feature Extractions\n",
    "- Create the final table with the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import ast\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_papers = pd.read_pickle('../Data/papers_clean.pkl')\n",
    "data_papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_papers['fos'] = data_papers.fos.apply(lambda x:  ast.literal_eval(x))\n",
    "data_papers['authors'] = data_papers.authors.apply(lambda x:  ast.literal_eval(x))\n",
    "data_papers['venue'] = data_papers.venue.apply(lambda x:  ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The list of features:\n",
    "\n",
    "* Topic Rank\n",
    "* Diversity\n",
    "* Productivity\n",
    "* H-index\n",
    "* Author Rank\n",
    "* Venue Rank\n",
    "* Maximum Past Influence of Authors (Past Influence of Authors)\n",
    "* Total Past Influence of Authors (Past Influence of Authors)\n",
    "* Maximum Past Influence of Venues (Past Influence of Venues)\n",
    "* Total Past Influence of Venues (Past Influence of Venues)\n",
    "* Versatility\n",
    "* Novelty\n",
    "* Sociality\n",
    "* Authority\n",
    "* Venue Centrality\n",
    "* First two years performance \n",
    "* Yearly citations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The features that need merge (like multi authors) will be calculated during feature file creation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps:\n",
    "\n",
    "- take the fos of all rows\n",
    "- take the unique values\n",
    "- create a --- matrix where the columns are the topics and each row is a paper, if topic exist => 1, otherwise 0\n",
    "- have a table with the topic as ID\n",
    "- calculate the score for each ID ==> \n",
    "- third column should be the rank\n",
    "\n",
    "INF(topic/d)= P(topic/d) * INF(paper)\n",
    "score = sum(Weight of fos in Document * n_citation of that document) in all papers\n",
    "rank = sort(score,ascending)\n",
    "\n",
    "Table: topics (topic, score, rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### forget for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics = pd.DataFrame(data_papers['fos'])\n",
    "# len(topics)\n",
    "# print(topics['fos'][0])\n",
    "# topics['fos'] = topics.fos.apply(lambda x: [i['name'] for i in x])\n",
    "# print(topics['fos'][0])\n",
    "# topic_names = topics['fos'].tolist()\n",
    "# topics = [ item for elem in topic_names for item in elem]\n",
    "# print(len(topics))\n",
    "# print(len(set(topics)))\n",
    "# from collections import Counter\n",
    "# for i in set(topics):\n",
    "#     print(topics.count(i), i)\n",
    "#topics[0:26]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps:\n",
    "\n",
    "- loop the FOS sum the Shannon index\n",
    "- Shannon index = -w Ln (w)\n",
    "\n",
    "The higher the value the more diversity; The lower the value the more focused the paper is\n",
    "\n",
    "https://en.wikipedia.org/wiki/Diversity_index\n",
    "\n",
    "https://en.wikipedia.org/wiki/Entropy_(information_theory)\n",
    "\n",
    "Table: paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- min value is 0.0\n",
    "- max value is 4.966 (but can be higher)\n",
    "- look up function: -i*ln(i) on Desmos\n",
    "- max value is 0.3679 at 0.3679 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diversity formula \n",
    "data_papers['diversity'] = data_papers['fos']\n",
    "data_papers['diversity'] = data_papers.diversity.apply(lambda x: sum([-i['w']*np.log(i['w']) for i in x if i['w'] > 0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analyze the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_papers['diversity'].head())\n",
    "print(data_papers['diversity'].describe())\n",
    "print(data_papers['n_citation'].corr(data_papers['diversity']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding paper IDs and citation to the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding author_id to paper table\n",
    "data_papers['author_id'] = data_papers['authors']\n",
    "data_papers['author_id'] = data_papers.author_id.apply(lambda x: [i['id'] for i in x])\n",
    "\n",
    "# Creating author table\n",
    "data_authors = data_papers[['id','authors', 'n_citation', 'fos']].to_numpy()\n",
    "data_authors = [{**j,'paper_ids':i[0],'n_citations':i[2],'FOS':i[3]} for i in data_authors for j in i[1]]\n",
    "data_authors_df = pd.DataFrame(data_authors)\n",
    "\n",
    "# Merging the duplicate authors (based on id)\n",
    "data_authors_df1 = data_authors_df.groupby(['id'])['paper_ids'].apply(list).reset_index()\n",
    "data_authors_df2 = data_authors_df.groupby(['id'])['n_citations'].apply(list).reset_index()\n",
    "data_authors_df3 = data_authors_df.groupby(['id'])['name'].first().reset_index()\n",
    "data_authors_df4 = data_authors_df.groupby(['id'])['org'].first().reset_index()\n",
    "data_authors_df5 = data_authors_df.groupby(['id'])['FOS'].apply(list).reset_index()\n",
    "\n",
    "data_authors_df = pd.merge(data_authors_df1, data_authors_df2, on = 'id', how = 'inner')\n",
    "data_authors_df = pd.merge(data_authors_df, data_authors_df3, on = 'id', how = 'inner')\n",
    "data_authors_df = pd.merge(data_authors_df, data_authors_df4, on = 'id', how = 'inner')\n",
    "data_authors_df = pd.merge(data_authors_df, data_authors_df5, on = 'id', how = 'inner')\n",
    "\n",
    "data_authors_df['FOS'] = data_authors_df.FOS.apply(lambda x: [j for i in x for j in i])\n",
    "\n",
    "data_authors_df.info()\n",
    "data_authors_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Venue table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding paper IDs and citation to the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding venue raw to paper table\n",
    "data_papers['venue_raw'] = data_papers['venue']\n",
    "data_papers['venue_raw'] = data_papers.venue_raw.apply(lambda x: x['raw'])\n",
    "\n",
    "# Creating venue table\n",
    "data_venues = data_papers[['id','venue', 'n_citation']].to_numpy()\n",
    "data_venues = [{**i[1],'paper_ids':i[0],'n_citations':i[2]} for i in data_venues]\n",
    "data_venues_df = pd.DataFrame(data_venues)\n",
    "\n",
    "# Merging the duplicate authors (based on id)\n",
    "data_venues_df1 = data_venues_df.groupby(['raw'])['paper_ids'].apply(list).reset_index()\n",
    "data_venues_df2 = data_venues_df.groupby(['raw'])['n_citations'].apply(list).reset_index()\n",
    "data_venues_df3 = data_venues_df.groupby(['raw'])['id'].first().reset_index()\n",
    "data_venues_df4 = data_venues_df.groupby(['raw'])['type'].first().reset_index()\n",
    "\n",
    "data_venues_df = pd.merge(data_venues_df1, data_venues_df2, on = 'raw', how = 'inner')\n",
    "data_venues_df = pd.merge(data_venues_df, data_venues_df3, on = 'raw', how = 'inner')\n",
    "data_venues_df = pd.merge(data_venues_df, data_venues_df4, on = 'raw', how = 'inner')\n",
    "\n",
    "data_venues_df.info()\n",
    "data_venues_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Productivity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the size of the paper IDs\n",
    "data_authors_df['productivity'] = data_authors_df['paper_ids']\n",
    "data_authors_df['productivity'] = data_authors_df.productivity.apply(lambda x: len(x))\n",
    "data_authors_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H-index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps:\n",
    "- use the code from before\n",
    "- calculate the final value during the feature file creation\n",
    "\n",
    "Table: Author (id, name, org, papers, productivity, h_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the H index using formula\n",
    "data_authors_df['H_index'] = data_authors_df['n_citations']\n",
    "data_authors_df['H_index'] = data_authors_df.H_index.apply(lambda x: sum(j >= i + 1 for i, j in enumerate(sorted(list(x), reverse=True))))\n",
    "data_authors_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps:\n",
    "- get citation of the papers the author has\n",
    "- take average\n",
    "- give rank \n",
    "\n",
    "Table: Author (id, name, org, papers, productivity, h_index, citations, ave_cite, author_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculates the average citations of the author and gives rank (descending)\n",
    "data_authors_df['average_citations'] = data_authors_df.n_citations.apply(mean)\n",
    "data_authors_df['author_rank'] = data_authors_df['average_citations'].rank(ascending = False)\n",
    "data_authors_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Venue Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "- get citation of the papers the venue has\n",
    "- take average\n",
    "- give rank\n",
    "\n",
    "\n",
    "table: Venue (id, raw (or name), papers(list of ids), citations, ave_citation, rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_venues_df['ave_citation'] = data_venues_df.n_citations.apply(mean)\n",
    "data_venues_df['venue_rank'] = data_venues_df['ave_citation'].rank(ascending = False)\n",
    "data_venues_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum Past Influence of Authors (Past Influence of Authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps:\n",
    "- max of citation\n",
    "\n",
    "Table: Author (id, name, org, papers, productivity, h_index, citations, ave_cite, author_rank, author_MPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the highest value in the citations \n",
    "data_authors_df['author_MPI'] = data_authors_df.n_citations.apply(max)\n",
    "data_authors_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Past Influence of Authors (Past Influence of Authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps:\n",
    "- total citation\n",
    "\n",
    "Table: Author (id, name, org, papers, productivity, h_index, citations, ave_cite, author_rank, author_MPI, author_TPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sum of the citations\n",
    "data_authors_df['author_TPI'] = data_authors_df.n_citations.apply(sum)\n",
    "data_authors_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum Past Influence of Venue (Past Influence of Venue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps:\n",
    "- max of citation\n",
    "\n",
    "table: Venue (id, raw (or name), papers(list of ids), citations, ave_citation, rank, venur_MPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_venues_df['venue_MPI'] = data_venues_df.n_citations.apply(max)\n",
    "data_venues_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Past Influence of Venue (Past Influence of Venue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps:\n",
    "- total citation\n",
    "\n",
    "table: Venue (id, raw (or name), papers(list of ids), citations, ave_citation, rank, venur_MPI, venur_TPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_venues_df['venue_TPI'] = data_venues_df.n_citations.apply(sum)\n",
    "data_venues_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Versatility "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps:\n",
    "- add column topics to table author (format: {'FOS1':average w, 'FOS2':average w, ...})\n",
    "- go over the FOS of papers of author\n",
    "    - if FOS in dic, value = value + w/productivity\n",
    "    - if not, create new key with FOS name and value FOS w/productivity\n",
    "- what we get per row {'FOS1':average w, 'FOS2':average w, ...}\n",
    "- loop the FOS sum the Shannon index\n",
    "- Shannon index = -w Ln (w)\n",
    "\n",
    "\n",
    "Table: Author (id, name, org, papers, productivity, h_index, citations, ave_cite, author_rank, author_MPI, author_TPI, versatility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns the list of list of dictionaries into list of dictionaries\n",
    "data_authors_df['versatility'] = data_authors_df['FOS']\n",
    "data_authors_df['FOS'] = data_authors_df.FOS.apply(lambda x: [{list(i.values())[0]:list(i.values())[1]} for i in x if list(i.values())[1] > 0.0])\n",
    "\n",
    "# Makes a unique list of all the topics with weights > 0.0\n",
    "data_authors_df['versatility'] = data_authors_df.versatility.apply(lambda x: list(set([i['name'] for i in x if list(i.values())[1] > 0.0])))\n",
    "# Turns the list into dictionary where the key is the topic and the value is a list of the weights for that topic\n",
    "data_authors_df['versatility'] = data_authors_df.apply(lambda x: {i:[list(j.values())[0] for j in x.FOS if list(j.keys())[0] == i] for i in x.versatility}, axis=1)\n",
    "# Gets the average of the weights of the topics\n",
    "data_authors_df['versatility'] = data_authors_df.apply(lambda x: {i:sum(x.versatility[i])/x.productivity for i in x.versatility}, axis = 1)\n",
    "# Diversity or versatility formula\n",
    "data_authors_df['versatility'] = data_authors_df.versatility.apply(lambda x: sum([-x[i]*np.log(x[i]) for i in x]))\n",
    "\n",
    "data_authors_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the feature set (model input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = data_papers.copy()\n",
    "features_df = features_df[['id','author_id','venue_raw', 'diversity','n_citation']]\n",
    "features_df2 = data_venues_df.copy()\n",
    "\n",
    "# Make the names of both columns the same and merge tables using the venue_raw as index\n",
    "features_df2.rename(columns={\"raw\":\"venue_raw\"},inplace=True)\n",
    "features_df2 = features_df2.drop(['id','type','ave_citation','paper_ids','n_citations'],axis =1)\n",
    "features_df = features_df.merge(features_df2, on = 'venue_raw', how = 'inner')\n",
    "features_df.info()\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_authors_df.copy()\n",
    "df['paper_ids'] = df.paper_ids.apply(lambda x: ','.join(map(str, x)))\n",
    "df = df.join(pd.DataFrame(df['paper_ids'].str.split(',', expand=True).stack().reset_index(level=1, drop=True),columns=['paper_ids' + ' '])).drop('paper_ids',1).rename(columns=str.strip).reset_index(drop=True)\n",
    "df['paper_ids'] = df['paper_ids'].str.strip()\n",
    "df = df.drop(columns=['n_citations','FOS', 'name', 'org', 'average_citations'])\n",
    "df = df.groupby('paper_ids').agg(list)\n",
    "df.reset_index(inplace=True)\n",
    "df['paper_ids'] = df['paper_ids'].astype(np.int64)\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"id\":\"idA\"},inplace=True)\n",
    "df.rename(columns={\"paper_ids\":\"id\"},inplace=True)\n",
    "features_df = features_df.merge(df, on = 'id', how = 'inner')\n",
    "features_df['productivity'] = features_df.productivity.apply(mean)\n",
    "features_df['H_index'] = features_df.H_index.apply(mean)\n",
    "features_df['author_rank'] = features_df.author_rank.apply(mean)\n",
    "features_df['author_MPI'] = features_df.author_MPI.apply(mean)\n",
    "features_df['author_TPI'] = features_df.author_TPI.apply(mean)\n",
    "features_df['versatility'] = features_df.versatility.apply(mean)\n",
    "features_df = features_df[['id', 'diversity', 'venue_rank', 'venue_MPI', 'venue_TPI', 'productivity', 'H_index', 'author_rank', 'author_MPI', 'author_TPI', 'versatility', 'n_citation']]\n",
    "features_df.info()\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Data/features_2001.'\n",
    "features_df.to_pickle(path + 'pkl')\n",
    "features_df.to_csv(path + 'csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
