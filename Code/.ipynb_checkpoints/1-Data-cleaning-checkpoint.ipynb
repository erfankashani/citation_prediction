{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Fetching and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will fetch the dataset and clean the necessary parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infoemation about the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is based on the AMiner V12 database. The database can be downloaded from https://www.aminer.org/citation\n",
    "\n",
    "The Predictors (Inputs): \n",
    "The Target (Output): 'n_citation' (number of citations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan of Attack:\n",
    "1- Analyze the Raw Data from papers.csv authors.cvs orgs.csv\n",
    "2- Analyze the Features from feature.csv and check the correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import ast\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_papers = pd.read_pickle('../Data/papers.pkl')\n",
    "data_papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Analyse each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of the data is: \", str(data_papers.shape))\n",
    "data_papers.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most of the data consist of Integers and Strings which is analyzed as object. Moreover, we know that n_citation is the type int64 but due to some data inconsistancies it is showing as an Object. We will try to clean the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first checking for Null values\n",
    "data_papers['n_citation'].isnull().sum()\n",
    "\n",
    "# fill the null vlaue with zero (0)\n",
    "data_papers = data_papers.dropna(subset = ['n_citation','year'])\n",
    "\n",
    "# check the data agian for null value\n",
    "data_papers['n_citation'].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the rows had the value of 'object' instead of the actual n_citaion value. We will drop that row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_papers = data_papers.drop(data_papers.loc[data_papers['n_citation'] == 'Journal'].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the rows have the citaion value in String format while others are in the Int format. We change all the values into integers for consistancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_papers['n_citation'] = pd.to_numeric(data_papers['n_citation'])\n",
    "data_papers['n_citation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_papers.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that the n_ciation type is an int64. Lets understand the basic statistical details like count, percentile, mean, std, max and min vlaues for the number of citaions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_papers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_papers['n_citation'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data, the mean value for citations is about 13 while the median is 3. this shows the data is fairly \n",
    "skewed. It shows half of the papers have lower than 3 citaions while 75% of the data have lower than 9 citation.\n",
    "\n",
    "- We suspect the max value 42080 can be an outlier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look into the largest values of the dataset and how many times they were repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = data_papers['n_citation'].sort_values(ascending=False).head(30)\n",
    "top_dict = list(zip(top.index, top.values))\n",
    "top_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Citations vs number they have been used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_df = pd.DataFrame(top_dict, columns =['n_citation', 'Frequency']) \n",
    "sns.lmplot( x='n_citation', y='Frequency', data = top_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Also Analyze the number of citations vs. the year of publication (this takes 5 min to complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# fig, ax = plt.subplots() \n",
    "# # get x and y data \n",
    "# year = data_papers['year']\n",
    "# citation = data_papers['n_citation'] \n",
    "# # create bar chart \n",
    "# ax.bar(year, citation) \n",
    "# # set title and labels \n",
    "# ax.set_title('Wine Review Scores') \n",
    "# ax.set_xlabel('Ciataion') \n",
    "# ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_papers.boxplot(column='year'))\n",
    "sns.boxplot(x=data_papers['n_citation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can visually see that most of the citations are close to 3-10 and there are skewed data between 10-15000. There is also a surge of data in abour 40000 citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the other columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analyze the column 'year'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_papers['year'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The years are represented as float64 format. We try to change them into int64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "column_name = 'year'\n",
    "\n",
    "# first check for null value in the column\n",
    "print(data_papers[data_papers[column_name].isnull()])\n",
    "\n",
    "# change the data type into int64\n",
    "data_papers[column_name] = data_papers[column_name].astype(np.int64)\n",
    "\n",
    "# check for unique values\n",
    "data_papers[column_name].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alias_ids\n",
    "After checking the alias_ids we concluded that it is not a consistant attribute in the database. there are 480 instances in a 80k sample.\n",
    "\n",
    "#### Fos\n",
    "fos has 385 NaN values which was decided to be dropped from the dataset.\n",
    "\n",
    "#### Venue\n",
    "venue has 784 NaN values and it was decided to be dropped from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analyze the column fos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_papers['fos'].isnull().sum())\n",
    "data_papers = data_papers.dropna(subset = ['fos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(data_papers['venue'].isnull().sum())\n",
    "data_papers = data_papers.dropna(subset = ['venue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving the data as pickel for next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_path = '../Data/papers_clean.pkl'\n",
    "data_papers.to_pickle(pickle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Try the same analysis on some other years, making sure the NaN values for other years are not too different.\n",
    "- Feature selection and extraction\n",
    "  - the graph features\n",
    "  - h-index etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Current Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = pd.read_csv('../Data/features100.csv')\n",
    "data_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the temp dataframes for later use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use 'feather' to save the dataFrames into binary which later can be loaded. much more efficient than saving using CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO-DOs\n",
    "read the below for feature engineering:\n",
    "- https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas to save the RAM space\n",
    "\n",
    "1- memory optimization step we can perform already at this point (assuming we know our table structure by now) \n",
    "   is specifying the columns data types during the import (via the dtype= input parameter).\n",
    "\n",
    "```\n",
    "df = pd.read_csv(‘data/clickstream_data.tsv’, \n",
    "    delimiter=’\\t’,\n",
    "    names=[‘coming_from’, ‘article’, ‘referrer_type’, ‘n’],\n",
    "    dtype={\n",
    "        ‘referrer_type’: ‘category’, \n",
    "        ’n’: ‘uint32’}\n",
    ")\n",
    "```\n",
    "\n",
    "2- using the python Dask library which uses parallel computing and supports dask dataframs.\n",
    "3- migrate the data into postgreSQL and write quaries. it can be connected into your jupyter notebook.\n",
    "\n",
    "\n",
    "source: https://towardsdatascience.com/how-to-handle-large-datasets-in-python-with-pandas-and-dask-34f43a897d55\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
