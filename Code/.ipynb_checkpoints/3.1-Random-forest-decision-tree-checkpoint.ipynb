{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vanilla-specialist",
   "metadata": {},
   "source": [
    "# Random Forest Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-reasoning",
   "metadata": {},
   "source": [
    "#### Add libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-boating",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ast\n",
    "import seaborn as sns\n",
    "from statistics import mean\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-alloy",
   "metadata": {},
   "source": [
    "#### Add the database 2005-2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-civilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for 2005 to 2010 \n",
    "dataset = pd.read_csv('../Data/features_2005_2010_yearly_citation.csv')\n",
    "dataset.head()\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for 2010 to extract the paper ids from year 2010\n",
    "id_2010 = pd.read_csv('../Data/papers2010.csv')\n",
    "id_2010.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-viking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for train and test \n",
    "ids = id_2010['id'].tolist() \n",
    "train = dataset[dataset['id'].isin(ids) == False]\n",
    "test = dataset[dataset['id'].isin(ids)]\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-cross",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ytrain and Xtrain\n",
    "y_train_1yr = train.iloc[:,14]\n",
    "y_train_2yr = train.iloc[:,15]\n",
    "y_train_3yr = train.iloc[:,16]\n",
    "y_train_4yr = train.iloc[:,17]\n",
    "y_train_5yr = train.iloc[:,18]\n",
    "y_train_7yr = train.iloc[:,19]\n",
    "y_train_10yr = train.iloc[:,20]\n",
    "\n",
    "X_train = train.iloc[:,2:12]\n",
    "print(y_train_5yr)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-merit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ytest and Xtest\n",
    "y_test_1yr = test.iloc[:,14]\n",
    "y_test_2yr = test.iloc[:,15]\n",
    "y_test_3yr = test.iloc[:,16]\n",
    "y_test_4yr = test.iloc[:,17]\n",
    "y_test_5yr = test.iloc[:,18]\n",
    "y_test_7yr = test.iloc[:,19]\n",
    "y_test_10yr = test.iloc[:,20]\n",
    "\n",
    "test_ids = test.iloc[:,1].copy()\n",
    "X_test = test.iloc[:,2:12]\n",
    "print(y_test_5yr)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-score",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = test_ids.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-sherman",
   "metadata": {},
   "source": [
    "#### Check the length of training-set vs. testing-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-foster",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dataset length:\", len(dataset))\n",
    "print(\"trainset length:\", len(train))\n",
    "print(\"testset length:\", len(test))\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-atlanta",
   "metadata": {},
   "source": [
    "# Model Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-jewel",
   "metadata": {},
   "source": [
    "#### random forest regressor after hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name):\n",
    "    model_name = '../Results/'+name+'.pkl'\n",
    "    pickle.dump(model,open(model_name,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_Predict(X_train, y_train, X_test, name):\n",
    "    regressor = RandomForestRegressor(n_estimators = 32, random_state = 0, min_samples_split = 2, min_samples_leaf = 2, max_features = 'auto', max_depth = 60, bootstrap =  True)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    # To save the models and reload them later uncomment the line below\n",
    "    #save_model(regressor, name)\n",
    "    y_predict = regressor.predict(X_test)\n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_1yr = train_and_Predict(X_train, y_train_1yr, X_test, 'rf_1yr')\n",
    "y_predict_1yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_2yr = train_and_Predict(X_train, y_train_2yr, X_test, 'rf_2yr')\n",
    "y_predict_2yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_3yr = train_and_Predict(X_train, y_train_3yr, X_test, 'rf_3yr')\n",
    "y_predict_3yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_4yr = train_and_Predict(X_train, y_train_4yr, X_test, 'rf_4yr')\n",
    "y_predict_4yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-wayne",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_5yr = train_and_Predict(X_train, y_train_5yr, X_test, 'rf_5yr')\n",
    "y_predict_5yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-compiler",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_7yr = train_and_Predict(X_train, y_train_7yr, X_test, 'rf_7yr')\n",
    "y_predict_7yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_10yr = train_and_Predict(X_train, y_train_10yr, X_test, 'rf_10yr')\n",
    "y_predict_10yr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-hughes",
   "metadata": {},
   "source": [
    "#### In case of loading a model from the results folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = pickle.load(open('../Results/rf_1yr.pkl','rb'))\n",
    "y_predict_new = new_model.predict(X_test)\n",
    "y_predict_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-tiffany",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-costume",
   "metadata": {},
   "source": [
    "#### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-yahoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(\"Results for 1 year prediction:\")\n",
    "print(\"R squared:\", r2_score(y_test_1yr, y_predict_1yr))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test_1yr, y_predict_1yr))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test_1yr, y_predict_1yr))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test_1yr, y_predict_1yr)))\n",
    "print('----------------------------------------')\n",
    "\n",
    "print(\"Results for 2 year prediction:\")\n",
    "print(\"R squared:\", r2_score(y_test_2yr, y_predict_2yr))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test_2yr, y_predict_2yr))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test_2yr, y_predict_2yr))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test_2yr, y_predict_2yr)))\n",
    "print('----------------------------------------')\n",
    "\n",
    "print(\"Results for 3 year prediction:\")\n",
    "print(\"R squared:\", r2_score(y_test_3yr, y_predict_3yr))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test_3yr, y_predict_3yr))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test_3yr, y_predict_3yr))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test_3yr, y_predict_3yr)))\n",
    "print('----------------------------------------')\n",
    "\n",
    "print(\"Results for 4 year prediction:\")\n",
    "print(\"R squared:\", r2_score(y_test_4yr, y_predict_4yr))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test_4yr, y_predict_4yr))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test_4yr, y_predict_4yr))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test_4yr, y_predict_4yr)))\n",
    "print('----------------------------------------')\n",
    "\n",
    "print(\"Results for 5 year prediction:\")\n",
    "print(\"R squared:\", r2_score(y_test_5yr, y_predict_5yr))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test_5yr, y_predict_5yr))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test_5yr, y_predict_5yr))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test_5yr, y_predict_5yr)))\n",
    "print('----------------------------------------')\n",
    "\n",
    "print(\"Results for 7 year prediction:\")\n",
    "print(\"R squared:\", r2_score(y_test_7yr, y_predict_7yr))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test_7yr, y_predict_7yr))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test_7yr, y_predict_7yr))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test_7yr, y_predict_7yr)))\n",
    "print('----------------------------------------')\n",
    "\n",
    "print(\"Results for 10 year prediction:\")\n",
    "print(\"R squared:\", r2_score(y_test_10yr, y_predict_10yr))\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test_10yr, y_predict_10yr))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test_10yr, y_predict_10yr))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test_10yr, y_predict_10yr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-lover",
   "metadata": {},
   "source": [
    "#### Histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-cross",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import PercentFormatter\n",
    "def graph_hist(y_train, y_test, y_predict, bins, title):\n",
    "    plt.hist([y_train, y_test, y_predict],range=(0,bins), bins = bins, density=True, label=['train', 'test', 'predict'])\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-california",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph_hist(y_train_1yr, y_test_1yr, y_predict_1yr, 10, \"1 year prediction\")\n",
    "graph_hist(y_train_2yr, y_test_2yr, y_predict_2yr, 25, \"2 year prediction\")\n",
    "graph_hist(y_train_3yr, y_test_3yr, y_predict_3yr, 25, \"3 year prediction\")\n",
    "graph_hist(y_train_4yr, y_test_4yr, y_predict_4yr, 25, \"4 year prediction\")\n",
    "graph_hist(y_train_5yr, y_test_5yr, y_predict_5yr, 25, \"5 year prediction\")\n",
    "graph_hist(y_train_7yr, y_test_7yr, y_predict_7yr, 25, \"7 year prediction\")\n",
    "graph_hist(y_train_10yr, y_test_10yr, y_predict_10yr, 25, \"10 year prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-casting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "packed-helping",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-discovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a paper\n",
    "X_test.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_demo = test.iloc[:,1:12]\n",
    "id_test = X_test_demo.iloc[310].id\n",
    "id_test = np.int64(id_test)\n",
    "id_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"title:\", str(id_2010.loc[id_2010['id'] == id_test].iloc[0]['title']))\n",
    "print(\"authors:\", str(id_2010.loc[id_2010['id'] == id_test].iloc[0]['authors']))\n",
    "print(\"year:\", str(id_2010.loc[id_2010['id'] == id_test].iloc[0]['year']))\n",
    "print(\"venue:\", str(id_2010.loc[id_2010['id'] == id_test].iloc[0]['venue']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the features\n",
    "y_test_1yr = test.iloc[310,12]\n",
    "y_test_2yr = test.iloc[310,13]\n",
    "y_test_5yr = test.iloc[310,14]\n",
    "y_test_10yr = test.iloc[310,15]\n",
    "\n",
    "X_test_demo = test.iloc[310,2:12]\n",
    "X_test_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.iloc[300,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-teddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "y_predict_1yr_demo = train_and_Predict(X_train, y_train_1yr, np.array( [X_test_demo,] ) )\n",
    "y_predict_1yr_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "y_predict_2yr_demo = train_and_Predict(X_train, y_train_2yr, np.array( [X_test_demo,] ) )\n",
    "y_predict_2yr_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-harmony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "y_predict_5yr_demo = train_and_Predict(X_train, y_train_5yr, np.array( [X_test_demo,] ) )\n",
    "y_predict_5yr_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "y_predict_10yr_demo = train_and_Predict(X_train, y_train_10yr, np.array( [X_test_demo,] ) )\n",
    "y_predict_10yr_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-merchandise",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance in eye of the random forest regresor model\n",
    "regressor = pickle.load(open('../Results/rf_1yr.pkl','rb'))\n",
    "feature_list = list(X_train.columns)\n",
    "# Get numerical feature importances\n",
    "importances = list(regressor.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of x locations for plotting\n",
    "x_values = list(range(len(importances)))\n",
    "# List of features sorted from most to least important\n",
    "sorted_importances = [importance[1] for importance in feature_importances]\n",
    "sorted_features = [importance[0] for importance in feature_importances]\n",
    "# Cumulative importances\n",
    "cumulative_importances = np.cumsum(sorted_importances)\n",
    "# Make a line graph\n",
    "plt.plot(x_values, cumulative_importances, 'g-')\n",
    "# Draw line at 95% of importance retained\n",
    "plt.hlines(y = 0.95, xmin=0, xmax=len(sorted_importances), color = 'r', linestyles = 'dashed')\n",
    "# Format x ticks and labels\n",
    "plt.xticks(x_values, sorted_features, rotation = 'vertical')\n",
    "# Axis labels and title\n",
    "plt.xlabel('Variable'); plt.ylabel('Cumulative Importance'); plt.title('Cumulative Importances');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-universal",
   "metadata": {},
   "source": [
    "Now we try to take the features which result in the top 95% of the importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after feature engineering\n",
    "new_train = train[['author_rank','venue_MPI','author_MPI','versatility','diversity','n_citation']].copy()\n",
    "new_test = test[['author_rank','venue_MPI','author_MPI','versatility','diversity','n_citation']].copy()\n",
    "new_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ytrain and Xtrain\n",
    "y_train_5yr = new_train.iloc[:,-1]\n",
    "X_train = new_train.iloc[:,1:-1]\n",
    "# Set ytest and Xtest\n",
    "y_test_2yr = new_test.iloc[:,-1]\n",
    "X_test = new_test.iloc[:,1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-sleep",
   "metadata": {},
   "source": [
    "## Correlation graph between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use seaborn for pair plots\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\", color_codes=True);\n",
    "# Create a custom color palete\n",
    "palette = sns.xkcd_palette(['dark blue', 'dark green', 'gold', 'orange'])\n",
    "# Make the pair plot with a some aesthetic changes\n",
    "sns.pairplot(train, diag_kind = 'kde', palette= palette, plot_kws=dict(alpha = 0.7),\n",
    "                   diag_kws=dict(shade=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-basket",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-newcastle",
   "metadata": {},
   "source": [
    "#### Checking the current random forest parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-secondary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(regressor.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-southeast",
   "metadata": {},
   "source": [
    "Lets optimize the following features:\n",
    "\n",
    "- n_estimators = number of trees in the foreset\n",
    "- max_features = max number of features considered for splitting a node\n",
    "- max_depth = max number of levels in each decision tree\n",
    "- min_samples_split = min number of data points placed in a node before the node is split\n",
    "- min_samples_leaf = min number of data points allowed in a leaf node\n",
    "- bootstrap = method for sampling data points (with or without replacement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-negative",
   "metadata": {},
   "source": [
    "#### Creating the parameter grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-organic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 100, num = 5)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-strategy",
   "metadata": {},
   "source": [
    "#### Random forest training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-craps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "regressor_optimized = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 10 different combinations, and use all available cores\n",
    "# cross validation is 3, therefore it uses 2 fold to train and 3rd to validate the results.\n",
    "rf_random = RandomizedSearchCV(estimator = regressor_optimized, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train_2yr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-following",
   "metadata": {},
   "source": [
    "#### The best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
