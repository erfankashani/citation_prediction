{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- Create the Tables for Authors, Venues\n",
    "- Perform Feature Extractions\n",
    "- Create the final table with the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import ast\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_papers = pd.read_pickle('../Data/papers_clean.pkl')\n",
    "data_papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_papers['fos'] = data_papers.fos.apply(lambda x:  ast.literal_eval(x))\n",
    "data_papers['authors'] = data_papers.authors.apply(lambda x:  ast.literal_eval(x))\n",
    "data_papers['venue'] = data_papers.venue.apply(lambda x:  ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The list of features:\n",
    "\n",
    "* Topic Rank\n",
    "* Diversity\n",
    "* Productivity\n",
    "* H-index\n",
    "* Author Rank\n",
    "* Venue Rank\n",
    "* Maximum Past Influence of Authors (Past Influence of Authors)\n",
    "* Total Past Influence of Authors (Past Influence of Authors)\n",
    "* Maximum Past Influence of Venues (Past Influence of Venues)\n",
    "* Total Past Influence of Venues (Past Influence of Venues)\n",
    "* Versatility\n",
    "* Novelty\n",
    "* Sociality\n",
    "* Authority\n",
    "* Venue Centrality\n",
    "* First two years performance \n",
    "* Yearly citations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The features that need merge (like multi authors) will be calculated during feature file creation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps:\n",
    "\n",
    "- take the fos of all rows\n",
    "- take the unique values\n",
    "- create a --- matrix where the columns are the topics and each row is a paper, if topic exist => 1, otherwise 0\n",
    "- have a table with the topic as ID\n",
    "- calculate the score for each ID ==> \n",
    "- third column should be the rank\n",
    "\n",
    "INF(topic/d)= P(topic/d) * INF(paper)\n",
    "score = sum(Weight of fos in Document * n_citation of that document) in all papers\n",
    "rank = sort(score,ascending)\n",
    "\n",
    "Table: topics (topic, score, rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### forget for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = pd.DataFrame(data_papers['fos'])\n",
    "len(topics)\n",
    "print(topics['fos'][0])\n",
    "topics['fos'] = topics.fos.apply(lambda x: [i['name'] for i in x])\n",
    "print(topics['fos'][0])\n",
    "topic_names = topics['fos'].tolist()\n",
    "topics = [ item for elem in topic_names for item in elem]\n",
    "print(len(topics))\n",
    "print(len(set(topics)))\n",
    "from collections import Counter\n",
    "for i in set(topics):\n",
    "    print(topics.count(i), i)\n",
    "#topics[0:26]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps:\n",
    "\n",
    "- loop the FOS sum the Shannon index\n",
    "- Shannon index = -w Ln (w)\n",
    "\n",
    "The higher the value the more diversity; The lower the value the more focused the paper is\n",
    "\n",
    "https://en.wikipedia.org/wiki/Diversity_index\n",
    "\n",
    "https://en.wikipedia.org/wiki/Entropy_(information_theory)\n",
    "\n",
    "Table: paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- min value is 0.0\n",
    "- max value is 4.966 (but can be higher)\n",
    "- look up function: -i*ln(i) on Desmos\n",
    "- max value is 0.3679 at 0.3679 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diversity formula \n",
    "data_papers['diversity'] = data_papers['fos']\n",
    "data_papers['diversity'] = data_papers.diversity.apply(lambda x: sum([-i['w']*np.log(i['w']) for i in x if i['w'] > 0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analyze the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_papers['diversity'].head())\n",
    "print(data_papers['diversity'].describe())\n",
    "print(data_papers['n_citation'].corr(data_papers['diversity']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding paper IDs and citation to the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding author_id to paper table\n",
    "data_papers['author_id'] = data_papers['authors']\n",
    "data_papers['author_id'] = data_papers.author_id.apply(lambda x: [i['id'] for i in x])\n",
    "\n",
    "#creating author table\n",
    "data_authors = data_papers[['id','authors', 'n_citation', 'fos']].to_numpy()\n",
    "data_authors = [{**j,\"paper_ids\":i[0],\"n_citations\":i[2],\"FOS\":i[3]} for i in data_authors for j in i[1]]\n",
    "data_authors_df = pd.DataFrame(data_authors)\n",
    "\n",
    "#Merging the duplicate authors (based on id)\n",
    "data_authors_df1 = data_authors_df.groupby(['id'])['paper_ids'].apply(list).reset_index()\n",
    "data_authors_df2 = data_authors_df.groupby(['id'])['n_citations'].apply(list).reset_index()\n",
    "data_authors_df3 = data_authors_df.groupby(['id'])['name'].first().reset_index()\n",
    "data_authors_df4 = data_authors_df.groupby(['id'])['org'].first().reset_index()\n",
    "data_authors_df5 = data_authors_df.groupby(['id'])['FOS'].apply(list).reset_index()\n",
    "\n",
    "data_authors_df = pd.merge(data_authors_df1, data_authors_df2, on = \"id\", how = \"inner\")\n",
    "data_authors_df = pd.merge(data_authors_df, data_authors_df3, on = \"id\", how = \"inner\")\n",
    "data_authors_df = pd.merge(data_authors_df, data_authors_df4, on = \"id\", how = \"inner\")\n",
    "data_authors_df = pd.merge(data_authors_df, data_authors_df5, on = \"id\", how = \"inner\")\n",
    "\n",
    "data_authors_df['FOS'] = data_authors_df.FOS.apply(lambda x: [j for i in x for j in i])\n",
    "\n",
    "data_authors_df.info()\n",
    "data_authors_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Venue table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding paper IDs and citation to the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding venue raw to paper table\n",
    "data_papers['venue_raw'] = data_papers['venue']\n",
    "data_papers['venue_raw'] = data_papers.venue_raw.apply(lambda x: x['raw'])\n",
    "\n",
    "#creating venue table\n",
    "data_venues = data_papers[['id','venue', 'n_citation']].to_numpy()\n",
    "data_venues = [{**i[1],\"paper_ids\":i[0],\"n_citations\":i[2]} for i in data_venues]\n",
    "data_venues_df = pd.DataFrame(data_venues)\n",
    "\n",
    "#Merging the duplicate authors (based on id)\n",
    "data_venues_df1 = data_venues_df.groupby(['raw'])['paper_ids'].apply(list).reset_index()\n",
    "data_venues_df2 = data_venues_df.groupby(['raw'])['n_citations'].apply(list).reset_index()\n",
    "data_venues_df3 = data_venues_df.groupby(['raw'])['id'].first().reset_index()\n",
    "data_venues_df4 = data_venues_df.groupby(['raw'])['type'].first().reset_index()\n",
    "\n",
    "data_venues_df = pd.merge(data_venues_df1, data_venues_df2, on = 'raw', how = \"inner\")\n",
    "data_venues_df = pd.merge(data_venues_df, data_venues_df3, on = \"raw\", how = \"inner\")\n",
    "data_venues_df = pd.merge(data_venues_df, data_venues_df4, on = \"raw\", how = \"inner\")\n",
    "\n",
    "data_venues_df.info()\n",
    "data_venues_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Productivity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps (table):\n",
    "\n",
    "- option 1:\n",
    "    - each row: [{'name':\"name1\",'id':\"id1\", 'org':\"org1\"},{'name':\"name2\",'id':, 'org':\"\"},...]\n",
    "    - a = [['name1','id1','org1'],...]\n",
    "    - a = list(set(a))\n",
    "    - a = [['name1','id1','org1', [paper_id1,paper_id2,..]],...]\n",
    "\n",
    "- option 2:\n",
    "    - [ [['name1','id1','org1'],[paper_id1]], ...]\n",
    "\n",
    "- option 3*:\n",
    "    - loop the papers and make this:\n",
    "    - {'id1':['name1','org1',[paper_id1,paper_id2,..]],..}\n",
    "\n",
    "steps (feature):\n",
    "- len(papers)\n",
    "- calculate the final value during the feature file creation\n",
    "\n",
    "Table: Author (id, name, org, papers, productivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets the size of the paper IDs\n",
    "data_authors_df['productivity'] = data_authors_df['paper_ids']\n",
    "data_authors_df['productivity'] = data_authors_df.productivity.apply(lambda x: len(x))\n",
    "data_authors_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H-index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps:\n",
    "- use the code from before\n",
    "- calculate the final value during the feature file creation\n",
    "\n",
    "Table: Author (id, name, org, papers, productivity, h_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the H index using formula\n",
    "data_authors_df['H_index'] = data_authors_df['n_citations']\n",
    "data_authors_df['H_index'] = data_authors_df.H_index.apply(lambda x: sum(j >= i + 1 for i, j in enumerate(sorted(list(x), reverse=True))))\n",
    "data_authors_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps:\n",
    "- get citation of the papers the author has\n",
    "- take average\n",
    "- give rank \n",
    "\n",
    "Table: Author (id, name, org, papers, productivity, h_index, citations, ave_cite, author_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculates the averge citations of the author and gives rank (descending)\n",
    "data_authors_df['average_citations'] = data_authors_df.n_citations.apply(mean)\n",
    "data_authors_df['author_rank'] = data_authors_df['average_citations'].rank(ascending = 0)\n",
    "data_authors_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Venue Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps (table):\n",
    "- same way as author\n",
    "- loop the papers and make this:\n",
    "- {'id1':['raw1',[paper_id1,paper_id2,..]],..}\n",
    "\n",
    "\n",
    "steps (feature):\n",
    "- get citation of the papers the venue has\n",
    "- take average\n",
    "- give rank\n",
    "\n",
    "\n",
    "table: Venue (id, raw (or name), papers(list of ids), citations, ave_citation, rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_venues_df[\"ave_citation\"] = data_venues_df.n_citations.apply(mean)\n",
    "data_venues_df[\"venue_rank\"] = data_venues_df[\"ave_citation\"].rank(na_option='bottom', method='max', ascending = False)\n",
    "data_venues_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum Past Influence of Authors (Past Influence of Authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps:\n",
    "- max of citation\n",
    "\n",
    "Table: Author (id, name, org, papers, productivity, h_index, citations, ave_cite, author_rank, author_MPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the highest value in the citations \n",
    "data_authors_df['author_MPI'] = data_authors_df.n_citations.apply(max)\n",
    "data_authors_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Past Influence of Authors (Past Influence of Authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps:\n",
    "- total citation\n",
    "\n",
    "Table: Author (id, name, org, papers, productivity, h_index, citations, ave_cite, author_rank, author_MPI, author_TPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sum of the citations\n",
    "data_authors_df['author_TPI'] = data_authors_df.n_citations.apply(sum)\n",
    "data_authors_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum Past Influence of Venue (Past Influence of Venue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps:\n",
    "- max of citation\n",
    "\n",
    "table: Venue (id, raw (or name), papers(list of ids), citations, ave_citation, rank, venur_MPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_venues_df[\"venue_MPI\"] = data_venues_df.n_citations.apply(max)\n",
    "data_venues_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Past Influence of Venue (Past Influence of Venue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps:\n",
    "- total citation\n",
    "\n",
    "table: Venue (id, raw (or name), papers(list of ids), citations, ave_citation, rank, venur_MPI, venur_TPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_venues_df[\"venue_TPI\"] = data_venues_df.n_citations.apply(sum)\n",
    "data_venues_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Versatility "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps:\n",
    "- add column topics to table author (format: {'FOS1':average w, 'FOS2':average w, ...})\n",
    "- go over the FOS of papers of author\n",
    "    - if FOS in dic, value = value + w/productivity\n",
    "    - if not, create new key with FOS name and value FOS w/productivity\n",
    "- what we get per row {'FOS1':average w, 'FOS2':average w, ...}\n",
    "- loop the FOS sum the Shannon index\n",
    "- Shannon index = -w Ln (w)\n",
    "\n",
    "\n",
    "Table: Author (id, name, org, papers, productivity, h_index, citations, ave_cite, author_rank, author_MPI, author_TPI, versatility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turns the list of list of dictionaries into list of dictionaries\n",
    "data_authors_df['versatility'] = data_authors_df['FOS']\n",
    "data_authors_df['FOS'] = data_authors_df.FOS.apply(lambda x: [{list(i.values())[0]:list(i.values())[1]} for i in x if list(i.values())[1] > 0.0])\n",
    "\n",
    "#makes a unique list of all the topics with weights > 0.0\n",
    "data_authors_df['versatility'] = data_authors_df.versatility.apply(lambda x: list(set([i['name'] for i in x if list(i.values())[1] > 0.0])))\n",
    "#turns the list into dictionary where the key is the topic and the value is a list of the weights for that topic\n",
    "data_authors_df['versatility'] = data_authors_df.apply(lambda x: {i:[list(j.values())[0] for j in x.FOS if list(j.keys())[0] == i] for i in x.versatility}, axis=1)\n",
    "#gets the average of the weights of the topics\n",
    "data_authors_df['versatility'] = data_authors_df.apply(lambda x: {i:sum(x.versatility[i])/x.productivity for i in x.versatility}, axis = 1)\n",
    "#diversity or versatility formula\n",
    "data_authors_df['versatility'] = data_authors_df.versatility.apply(lambda x: sum([-x[i]*np.log(x[i]) for i in x]))\n",
    "\n",
    "data_authors_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the feature set (model input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
